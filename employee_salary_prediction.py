# -*- coding: utf-8 -*-
"""Employee Salary Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19nodu2myD1hmRro6vV5pT1oqDlc59WgB
"""

# Commented out IPython magic to ensure Python compatibility.
#importing necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

#loading the datasets
train_dataset = pd.read_csv('/content/drive/MyDrive/train_dataset.csv')
train_salaries = pd.read_csv('/content/drive/MyDrive/train_salaries.csv')

train_dataset.head()

train_salaries.head()

#merging both datasets
df = pd.merge(train_dataset, train_salaries, on='jobId')[0:50000]
df.head()

#dropping unnecessary columns
df = df.drop(['jobId', 'companyId'], axis=1)

#checking basic info
df.info()

#relationship between different variables
sns.pairplot(df)

plt.figure(figsize=(20,8))

plt.subplot(2,2,1)
sns.histplot(data=df, x='jobType', y='salary')

plt.subplot(2,2,2)
sns.histplot(data=df, x='degree', y='salary')

plt.subplot(2,2,3)
sns.histplot(data=df, x='major', y='salary')

plt.subplot(2,2,4)
sns.histplot(data=df, x='industry', y='salary')

#checking unique elements in object type columns
print('Job Type:', df['jobType'].unique(), '\n', 'Degree:', df['degree'].unique(), '\n', 'Major:', df['major'].unique(), '\n', 'Industry:', df['industry'].unique())

#converting categorical variables to numerical
df = pd.get_dummies(df)
df.columns

#scaling the data
df = pd.DataFrame(data=StandardScaler().fit_transform(df), columns=df.columns)
df.head()

#Defining feature set and target variable
X = df.drop('salary', axis=1).values
y = df['salary'].values

#splitting the dataset into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#fitting a Linear Regression model on our data and checking how good it is at predicting
LR = LinearRegression()
LR.fit(X_train, y_train)
y_pred = LR.predict(X_test)
RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
R2 = r2_score(y_test, y_pred)
print('RMSE:',RMSE,'\n','R2 score:',R2)

#fitting a Random Forest Regressor model
RF = RandomForestRegressor()
RF.fit(X_train, y_train)
y_pred = RF.predict(X_test)
RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
R2 = r2_score(y_test, y_pred)
print('RMSE:', RMSE, '\n', 'R2 score:', R2)

#hyperparameter tuning using RandomizedSearchCV
RS = RandomizedSearchCV(estimator=RF, param_distributions={'n_estimators':[25, 50, 100, 150, 200], 'max_features':['auto', 'sqrt', 'log2'], 'min_samples_split':range(1,30,1), 'min_samples_leaf':range(1,30,1), 'max_depth':range(1,20,1),'bootstrap':[True, False]}, cv=5)
RS.fit(X_train, y_train)
RS.best_params_

#using best parameters from hyperparameter tuning
RF1= RandomForestRegressor(n_estimators=200,  min_samples_split=20, min_samples_leaf=10,  max_features='auto', max_depth=13, bootstrap=True)
RF1.fit(X_train, y_train)
y_pred = RF1.predict(X_test)
RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
R2 = r2_score(y_test, y_pred)
print('RMSE:', RMSE, '\n', 'R2 score:', R2)

#AdaBoost
AB = AdaBoostRegressor(LR)
AB.fit(X_train, y_train)
y_pred = AB.predict(X_test)
RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
R2 = r2_score(y_test, y_pred)
print('RMSE:', RMSE, '\n', 'R2 score:',R2)

#GradientBoost
GB = GradientBoostingRegressor()
GB.fit(X_train, y_train)
y_pred = GB.predict(X_test)
RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
R2 = r2_score(y_test, y_pred)
print('RMSE:', RMSE, '\n', 'R2 score:',R2)

#hyperparameter tuning using RandomizedSearchCV
RS = RandomizedSearchCV(estimator=GB, param_distributions={'n_estimators':[25, 50, 100, 150, 200], 'learning_rate' : [1, 0.5, 0.25, 0.1, 0.05, 0.01], 'min_samples_split':range(1,30,2), 'min_samples_leaf':range(1,30,2), 'max_depth':range(1,20,1)}, cv=5)
RS.fit(X_train, y_train)
RS.best_params_

#using best parameters from hyperparameter tuning
GB1 = GradientBoostingRegressor(**RS.best_params_)
GB1.fit(X_train, y_train)
y_pred = GB1.predict(X_test)
RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
R2 = r2_score(y_test, y_pred)
print('RMSE:', RMSE, '\n', 'R2 score:',R2)

#XGBoost
XG = XGBClassifier()
XG.fit(X_train, y_train)
y_pred = XG.predict(X_test)
RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
R2 = r2_score(y_test, y_pred)
print('RMSE:', RMSE, '\n', 'R2 score:',R2)

#Gradient Boost Regressor model gives the highest accuracy of 75%.